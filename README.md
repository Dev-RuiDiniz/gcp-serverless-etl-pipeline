<h1 align="center">GCP Serverless ETL Pipeline</h1>

<p align="center">
  <strong>Pipeline ETL Serverless usando Google Cloud Functions, BigQuery e Python</strong><br>
  ExtraÃ§Ã£o de dados de API pÃºblica â†’ TransformaÃ§Ã£o com Pandas â†’ Carga otimizada no BigQuery â†’ VisualizaÃ§Ã£o no Looker Studio.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue" />
  <img src="https://img.shields.io/badge/Google%20Cloud-BigQuery-orange" />
  <img src="https://img.shields.io/badge/Cloud%20Functions-Serverless-success" />
  <img src="https://img.shields.io/badge/Status-Production%20Ready-brightgreen" />
</p>

---

# ğŸ“Œ Sobre o Projeto

Este projeto implementa um **ETL totalmente Serverless na Google Cloud**, utilizando:

- **Google Cloud Functions** â†’ execuÃ§Ã£o sob demanda  
- **Requests + Retry** â†’ extraÃ§Ã£o resiliente  
- **Pandas** â†’ transformaÃ§Ã£o e limpeza  
- **BigQuery** â†’ armazenamento analÃ­tico  
- **Looker Studio** â†’ dashboards automatizados  
- **Logging estruturado (JSON)** â†’ Cloud Logging  
- **Secret Manager** â†’ gestÃ£o segura de credenciais

Ideal para portfÃ³lio de **Engenharia de Dados**, mostrando domÃ­nio prÃ¡tico do ecossistema GCP.

---

# ğŸ›ï¸ Arquitetura

<p align="center">
  <img src="deploy/architecture_diagram.png" width="700">
</p>

Fluxo:
1. A Cloud Function Ã© acionada (HTTP Trigger ou Scheduler).  
2. O Extractor consome uma API externa (ex: IBGE).  
3. O Transformer limpa/normaliza os dados com Pandas.  
4. O Loader envia para o BigQuery com create-if-not-exists.  
5. Os dados ficam disponÃ­veis para dashboards no Looker Studio.  

---

# ğŸ—‚ï¸ Estrutura do Projeto

```txt
.
â”œâ”€â”€ README.md
â”œâ”€â”€ estrutura.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ .gitignore

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ cloud_function_handler.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ exceptions.py
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extractor.py
â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â””â”€â”€ loader.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api_service.py
â”‚   â”‚   â”œâ”€â”€ bigquery_service.py
â”‚   â”‚   â””â”€â”€ secrets_service.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ record_model.py
â”‚   â”‚   â””â”€â”€ schema_definition.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ validators.py
â”‚       â””â”€â”€ serializers.py

â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extractor.py
â”‚   â”œâ”€â”€ test_transformer.py
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_bigquery_service.py
â”‚   â”œâ”€â”€ test_api_service.py
â”‚   â””â”€â”€ test_cloud_handler.py

â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ gcloud_instructions.md
â”‚   â””â”€â”€ architecture_diagram.png

â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ bigquery_schema.md
    â”œâ”€â”€ api_reference.md
    â””â”€â”€ looker_setup.md
```
---

## ğŸš€ Executando Localmente

1. Criar ambiente virtual
```bash
Copiar cÃ³digo
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```
2. Instalar dependÃªncias
```bash
Copiar cÃ³digo
pip install -r requirements.txt
```
3. Criar arquivo .env
```env
Copiar cÃ³digo
GCP_PROJECT_ID=seu_projeto
BIGQUERY_DATASET=etl_dataset
BIGQUERY_TABLE=api_data
```
4. Executar o ETL
```bash
Copiar cÃ³digo
python src/main.py
```
---

## â˜ï¸ Deploy no Google Cloud Functions

1. Autenticar
```bash
Copiar cÃ³digo
gcloud auth login
```
2. Usar script automÃ¡tico
```bash
Copiar cÃ³digo
bash deploy/deploy.sh
```
3. Deploy manual
```bash
Copiar cÃ³digo
gcloud functions deploy gcp_etl_pipeline \
  --runtime python311 \
  --trigger-http \
  --entry-point main \
  --region southamerica-east1 \
  --set-env-vars GCP_PROJECT_ID=xxx,BIGQUERY_DATASET=xxx,BIGQUERY_TABLE=xxx \
  --allow-unauthenticated
```

---

## ğŸ§ª Testes
Os testes usam pytest + mocks.

Rodar:

```bash
Copiar cÃ³digo
pytest -v
Coverage:
```
```bash
Copiar cÃ³digo
pytest --cov=src
```

---

## ğŸ“Š VisualizaÃ§Ã£o no Looker Studio

Acesse: https://lookerstudio.google.com/

Crie uma nova fonte â†’ BigQuery

Conecte ao dataset/tabela do pipeline

Publique seu dashboard

DocumentaÃ§Ã£o completa em: docs/looker_setup.md

---

## ğŸ”’ SeguranÃ§a

- Secrets nunca ficam no cÃ³digo
- Uso de Secret Manager (services/secrets_service.py)
- Logging em formato JSON (compatÃ­vel com Cloud Logging)
- BigQuery com schema fixo (models/schema_definition.py)

---

## ğŸ§­ Roadmap

- Criar CI/CD com GitHub Actions
- Implementar CDC com Debezium
- Criar segunda pipeline incremental
- Adicionar Airflow (Composer) como orquestrador opcional

---

## ğŸ¤ ContribuiÃ§Ã£o

Pull requests sÃ£o bem-vindos!
Para contribuir:

```bash
Copiar cÃ³digo
git checkout -b feature/nome-da-feature
git commit -m "DescriÃ§Ã£o"
git push origin feature/nome-da-feature
```
---

# ğŸ“§ Contato

**Rui Francisco de Paula InÃ¡cio Diniz**

Engenheiro de Software â€¢ Analista de Dados
- LinkedIn: https://www.linkedin.com/in/
- GitHub: https://github.com/Dev-RuiDiniz

<h3 align="center">ğŸ”¥ Serverless, escalÃ¡vel e pronto para produÃ§Ã£o!</h3> ```